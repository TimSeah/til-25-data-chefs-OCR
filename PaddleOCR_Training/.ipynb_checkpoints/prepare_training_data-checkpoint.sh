#!/bin/bash

# This script prepares data for PaddleOCR training.
# Assumes it's run from /home/jupyter/PaddleOCR_Training/
# Uses a predefined character dictionary and max_text_length.
# Current User: lolkabash
# Current Date: 2025-05-25

# Exit on any error
set -e

# --- Configuration ---
# Base directory for the raw image and HOCR data
RAW_DATA_BASE_DIR="/home/jupyter/advanced/ocr" # Assuming images and HOCR are here
IMAGE_DIR="${RAW_DATA_BASE_DIR}"
HOCR_DIR="${RAW_DATA_BASE_DIR}" # Or a specific subdirectory if HOCRs are separate

# Base directory for OCR outputs (training labels, dicts)
OCR_OUTPUT_DIR="/home/jupyter/PaddleOCR_Training/ocr_output"
SCRIPTS_DIR="/home/jupyter/PaddleOCR_Training/scripts" # Where your Python helper scripts are

# Path to the CSV file generated by data_preprocess.py
LINE_LABELS_CSV="${OCR_OUTPUT_DIR}/line_labels.csv"

# Path to your predefined character dictionary (source)
# IMPORTANT: Place your char.txt (with 96 characters) at this location
PREDEFINED_CHAR_DICT_SOURCE_PATH="${SCRIPTS_DIR}/char.txt"

# Path for the character dictionary to be used by PaddleOCR (target)
CHAR_DICT_FILE="${OCR_OUTPUT_DIR}/custom_char_dict.txt"

# Predefined maximum text length
MAX_TEXT_LENGTH=128

# --- Script Execution ---

echo "--- Running Data Preprocessing (HOCR to Line Labels CSV) ---"
# Call data_preprocess.py with the required arguments
python "${SCRIPTS_DIR}/../data_preprocess.py" \
    "$IMAGE_DIR" \
    "$HOCR_DIR" \
    "$LINE_LABELS_CSV"
# Note: Assuming data_preprocess.py is one level up from SCRIPTS_DIR, in the main PaddleOCR_Training dir.
# If data_preprocess.py is in SCRIPTS_DIR, change the path to:
# python3 "${SCRIPTS_DIR}/data_preprocess.py" ...

if [ ! -f "$LINE_LABELS_CSV" ]; then
    echo "Error: data_preprocess.py did not create $LINE_LABELS_CSV. Exiting."
    exit 1
fi
echo "data_preprocess.py completed. Output CSV: $LINE_LABELS_CSV"
echo ""

echo "--- Using Predefined Character Dictionary ---"
if [ ! -f "$PREDEFINED_CHAR_DICT_SOURCE_PATH" ]; then
    echo "Error: Predefined character dictionary not found at $PREDEFINED_CHAR_DICT_SOURCE_PATH."
    echo "Please place your char.txt file there."
    exit 1
fi
cp "$PREDEFINED_CHAR_DICT_SOURCE_PATH" "$CHAR_DICT_FILE"
if [ ! -f "$CHAR_DICT_FILE" ]; then
    echo "Error: Failed to copy predefined character dictionary to $CHAR_DICT_FILE. Exiting."
    exit 1
fi
echo "Using predefined character dictionary: $CHAR_DICT_FILE"
echo "Number of characters in dictionary: $(wc -l < "$CHAR_DICT_FILE")"
echo ""

echo "--- Using Predefined Maximum Text Length ---"
echo "Maximum text length set to: $MAX_TEXT_LENGTH"
echo ""

echo "--- Converting CSV to PaddleOCR Label Format ---"
python "${SCRIPTS_DIR}/convert_csv_to_paddle_labels.py" \
    "$LINE_LABELS_CSV" \
    "$OCR_OUTPUT_DIR" \
    --char_dict "$CHAR_DICT_FILE" \
    --max_text_length "$MAX_TEXT_LENGTH" # Pass max_text_length if your script supports it
echo ""

echo "--- Data Preparation for PaddleOCR Training Complete ---"
echo "Outputs are in: $OCR_OUTPUT_DIR"
echo "Key files to be used:"
echo "  - Character Dictionary: $CHAR_DICT_FILE (predefined)"
echo "  - Training Labels: ${OCR_OUTPUT_DIR}/rec_gt_train.txt"
echo "  - Evaluation Labels: ${OCR_OUTPUT_DIR}/rec_gt_eval.txt"
echo "  - Maximum Text Length: $MAX_TEXT_LENGTH (predefined)"
echo ""
echo "Next steps:"
echo "1. Review the generated label files: ${OCR_OUTPUT_DIR}/rec_gt_train.txt and ${OCR_OUTPUT_DIR}/rec_gt_eval.txt."
echo "   Ensure they were filtered correctly based on '$CHAR_DICT_FILE' and max length $MAX_TEXT_LENGTH."
echo "2. Update your PaddleOCR training configuration YAML file with:"
echo "   - Character dictionary path: character_dict_path: $CHAR_DICT_FILE"
echo "   - Training data path (e.g., Train.dataset.label_file_list): [\"${OCR_OUTPUT_DIR}/rec_gt_train.txt\"]"
echo "   - Evaluation data path (e.g., Eval.dataset.label_file_list): [\"${OCR_OUTPUT_DIR}/rec_gt_eval.txt\"]"
echo "   - Max text length (e.g., Global.max_text_length or similar): $MAX_TEXT_LENGTH"
echo "3. Ensure the character dictionary at '$CHAR_DICT_FILE' is the one used for inference if you deploy this fine-tuned model."
